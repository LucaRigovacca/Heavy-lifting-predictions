---
title: 'Coursera Prediction Project: Heavy Lifting Classification'
author: "Luca Rigovacca"
date: "24 June 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, in order to predict in which way they where doing the exercise ("classe" variable in the dataset). The data comes from: http://groupware.les.inf.puc-rio.br/har.

After cleaning the data, we split the available data into train and validation set, and fit three prediction models (one linear discriminant analysis and two random forest). The last two models have equal predictions on the test set, and out-of-sample accuracy of more than 99% on the validation set.


## Details of the analysis

We start by loading the data from the current directory:
```{r, echo = TRUE}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

We can check for NA values, and print their proportion for each column by means of the following function.
```{r, echo = TRUE}
naProp <- function(x){
            sum(is.na(x))/length(x)
}
```
For the provided train set, either all data are present, or almost 98% are missing. For the test set, either their are all present or all missing. We thus select only the features that appear in the test set. After this operation, there are no more missing data in the reduced train set. We also remove the columns corresponding to row number, data and time to both train and test set (although time data could lead to some information, we would like to predict only through information about the user and his/her movements). All these operations on the datasets can be performed by the code below, and in output we are left with the data that we will analyze.
```{r, echo = TRUE}
#Proportions of NAs:
naTrain <- sapply(train, naProp)
naTest <- sapply(test, naProp)

#We keep only the columns without missing data in the test set, and we remove row number and date/time information. For the test set we need to make sure to keep the "problem-id" column instead of the "classe" one.
retained_features_train <- names(train[,naTest == 0])[-c(1,3:7)]
retained_features_test <- c(retained_features_train[-54],names(test)[160])

retainedTrain <- train[, retained_features_train]
retainedTest <- test[, retained_features_test]
```

We are ready to split our train set into a validation (20% of data) and a proper training set (80% of data). We will train all our models in the training set, and compare their performance on the validation set before predicting on the test set.
```{r, echo = TRUE, results = 'hide'}
library(caret)
inTrain <- createDataPartition(retainedTrain$classe, list = FALSE, p = 0.8)
trainset <- retainedTrain[inTrain,]
validset <- retainedTrain[-inTrain,]
```


Now we train three models, by pre-processing the data so that they have zero mean and unit standard deviation. We start with a linear discriminant analysis, using a 10-fold cross validation in the train set. We will see that the accuracy is rather low, so we move to more complicated random forest models. The first of these compute 20 trees, with a 5-fold cross validation in the train set, while the second compute 100 trees on the whole training set. 
```{r, echo = TRUE, eval=FALSE}
train_control<- trainControl(method="cv", number=10)
fitLDA <- train(classe ~ ., data = trainset, method = "lda", preProc = c("center","scale"), trControl = train_control)
predLDA <- predict(fitLDA, newdata = validset)

train_control<- trainControl(method="cv", number=5)
fitRFCV <- train(classe ~ ., data = trainset, method = "rf", ntree = 20, preProc = c("center","scale"), trControl = train_control)
predRFCV <- predict(fitRFCV, newdata = validset)

fitRF100 <- train(classe ~ ., data = trainset, method = "rf", ntree = 100, preProc = c("center","scale"))
predRF100 <- predict(fitRF100, newdata = validset)
```

For each model, we can study the in-sample accuracy and also the out-of-sample (OOS) accuracy evaluated on the validation set. The following table also provides the 95% confidence interval on the latter. The two random forest models have similar high OOS accuracies, with a lower bound on their estimated value larger than 99%. Furthermore, the OOS accuracy is larger than the in-sample one, suggesting that the two models are not overfitting.
```{r, include=TRUE, echo=FALSE}
load("C:\\Users\\Luca\\Documents\\Rprogramming\\projects\\prediction\\project\\computed_variables.Rdata")
````
```{r, echo = TRUE}
trainAccuracy <- function(fit_list){
                    sapply(fit_list,function(fit){max(fit$results$Accuracy)})
}
validAccuracy <- function(pred_list, truth){
                    sapply(pred_list,function(pred)
                      {confusionMatrix(pred,truth)$overall["Accuracy"]})
}
validAccLower <- function(pred_list, truth){
                    sapply(pred_list,function(pred)
                      {confusionMatrix(pred,truth)$overall["AccuracyLower"]})
}
validAccUpper <- function(pred_list, truth){
                    sapply(pred_list,function(pred)
                      {confusionMatrix(pred,truth)$overall["AccuracyUpper"]})
}
  
fits <- list(fitLDA,fitRFCV,fitRF100)
preds <- list(predLDA,predRFCV,predRF100)
validation_classes <- validset$classe

models_df <- data.frame(model = c("LDA", "RFCV", "RF100"), 
                        train_accuracy = trainAccuracy(fits), 
                        valid_accuracy = validAccuracy(preds,validation_classes),
                        valid_accuracy_low = validAccLower(preds,validation_classes),
                        valid_accuracy_up = validAccUpper(preds,validation_classes)
                        )    

models_df
```

The final predictions for the test set are the same for the two models based on random forest, and are:
```{r, echo = FALSE, eval = FALSE}
testpredRF100 <- predict(fitRF100, newdata = retainedTest)
testpredRFCV <- predict(fitRFCV, newdata = retainedTest)
outputRF100 <- data.frame(id = retainedTest$problem_id, classe = testpredRF100)
outputRFCV <- data.frame(id = retainedTest$problem_id, classe = testpredRFCV)
all(outputRF100==outputRFCV)
```
```{r, echo = FALSE}
outputRFCV

```


